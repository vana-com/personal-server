[tool.poetry]
name = "selfie"
version = "0.1.0"
description = "Data mixin for LLMs"
authors = ["Vana <services@vana.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = ">=3.9,<4.0" # >=3.9 for numpy, <4 for llama-index
beautifulsoup4 = "^4.12.3"
colorlog = "^6.8.2"
fastapi = "^0.109.0"
uvicorn = "^0.27.0"
humanize = "^4.9.0"
llama-cpp-python = ">=0.2.75"  # Updated to match txtai's requirement
litellm = "1.37.16"  # Newer versions than this break macOS builds: https://github.com/BerriAI/litellm/issues/2607
txtai = {version = "^8.0.0", extras = ["pipeline-llm"]}
sse-starlette = "^2.0.0"
llama-index = "^0.10.4"
numpy = "<2.0.0"  # Updated to be compatible with llama-index-core
html2text = "^2020.1.16"
peewee = "^3.17.0"
pypdf = "^4.0.1"
python-multipart = "^0.0.9"
ngrok = "^1.0.0"
python-dotenv = "^1.0.1"
pillow = "^10.2.0"
pyqt6 = "^6.6.1"
swig = "^4.2.1"
torch = ">=2.0, <2.2.1"

# Experimental GPU Features
auto-gptq = { version = "^0.6.0", optional = true }
optimum = { version = "^1.16.2", optional = true }
autoawq = { version = "^0.1.8", optional = true }

[tool.poetry.extras]
gpu = ["auto-gptq", "optimum", "autoawq"]

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
